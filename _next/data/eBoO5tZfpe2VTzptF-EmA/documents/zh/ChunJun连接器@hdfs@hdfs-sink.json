{"pageProps":{"content":"\n              <a id=\"HDFS Sink\" style='display: block; height: 35px;'></a>\n              <h1>\n                HDFS Sink\n              </h1>\n            \n              <a id=\"一、介绍\" style='display: block; height: 35px;'></a>\n              <h2>\n                一、介绍\n              </h2>\n            <p>HDFS插件支持直接从配置的HDFS路径上读取及写入TextFile、Orc、Parquet类型的文件，一般配合HIve表使用。如：读取Hive表某分区下所有数据，实质是读取Hive表对应分区的HDFS路径下的数据文件；将数据写入Hive表某分区，实质是直接将数据文件写入到对应分区的HDFS路径下；HDFS插件不会对Hive表进行任何DDL操作。</p>\n<p>HDFS Sink在开启checkpoint时会使用二阶段提交，预提交时将.data目录中生成的数据文件复制到正式目录中并标记复制的数据文件，提交阶段删除.data目录中标记的数据文件，回滚时删除正式目录中标记的数据文件。</p>\n\n              <a id=\"二、支持版本\" style='display: block; height: 35px;'></a>\n              <h2>\n                二、支持版本\n              </h2>\n            <p>Hadoop 2.x、Hadoop 3.x</p>\n\n              <a id=\"三、插件名称\" style='display: block; height: 35px;'></a>\n              <h2>\n                三、插件名称\n              </h2>\n            <table>\n<thead>\n<tr>\n<th>Sync</th>\n<th>hdfssink、hdfswriter</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SQL</td>\n<td>hdfs-x</td>\n</tr>\n</tbody></table>\n\n              <a id=\"四、参数说明\" style='display: block; height: 35px;'></a>\n              <h2>\n                四、参数说明\n              </h2>\n            \n              <a id=\"1、Sync\" style='display: block; height: 35px;'></a>\n              <h3>\n                1、Sync\n              </h3>\n            <ul>\n<li><p><strong>path</strong></p>\n<ul>\n<li>描述：写入的数据文件的路径</li>\n<li>注意：真正写入的文件路径是 path/fileName</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>fileName</strong></p>\n<ul>\n<li>描述：数据文件目录名称,可以传递分区(eg: pt=2022)</li>\n<li>注意：真正写入的文件路径是 path/fileName</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>writeMode</strong></p>\n<ul>\n<li>描述：HDFS Sink写入前数据清理处理模式：<ul>\n<li>append：追加</li>\n<li>overwrite：覆盖</li>\n</ul>\n</li>\n<li>注意：overwrite模式时会删除hdfs当前目录下的所有文件</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：append<br /></li>\n</ul>\n</li>\n<li><p><strong>fileType</strong></p>\n<ul>\n<li>描述：文件的类型，目前只支持用户配置为<code>text</code>、<code>orc</code>、<code>parquet</code><ul>\n<li>text：textfile文件格式</li>\n<li>orc：orcfile文件格式</li>\n<li>parquet：parquet文件格式</li>\n</ul>\n</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>defaultFS</strong></p>\n<ul>\n<li>描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>column</strong></p>\n<ul>\n<li>描述：需要读取的字段</li>\n<li>注意：不支持*格式</li>\n<li>格式：<pre><code class=\"language-text\"><span class=\"hljs-attr\">&quot;column&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-punctuation\">{</span>\n    <span class=\"hljs-attr\">&quot;name&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;col&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;type&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;string&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;index&quot;</span><span class=\"hljs-punctuation\">:</span><span class=\"hljs-number\">1</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;isPart&quot;</span><span class=\"hljs-punctuation\">:</span><span class=\"hljs-literal\"><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;format&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;yyyy-MM-dd hh:mm:ss&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;value&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;value&quot;</span>\n<span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">]</span>\n</code></pre>\n</li>\n</ul>\n</li>\n<li><p>属性说明:</p>\n<ul>\n<li>name：必选，字段名称</li>\n<li>type：必选，字段类型，需要和数据文件中实际的字段类型匹配</li>\n<li>index：非必选，字段在所有字段中的位置索引，从0开始计算，默认为-1，按照数组顺序依次读取，配置后读取指定字段列</li>\n<li>isPart：非必选，是否是分区字段，如果是分区字段，会自动从path上截取分区赋值，默认为fale</li>\n<li>format：非必选，按照指定格式，格式化日期</li>\n<li>value：非必选，常量字段，将value的值作为常量列返回</li>\n</ul>\n</li>\n<li><p>必选：是</p>\n</li>\n<li><p>参数类型：数组</p>\n</li>\n<li><p>默认值：无</p>\n<br />\n</li>\n<li><p><strong>hadoopConfig</strong></p>\n<ul>\n<li>描述：集群HA模式时需要填写的core-site.xml及hdfs-site.xml中的配置，开启kerberos时包含kerberos相关配置</li>\n<li>必选：否</li>\n<li>参数类型：Map&lt;String, Object&gt;</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>fieldDelimiter</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的分隔符</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：<code>\\001</code><br /></li>\n</ul>\n</li>\n<li><p><strong>fullColumnName</strong></p>\n<ul>\n<li>描述：写入的字段名称</li>\n<li>必须：否</li>\n<li>字段类型：list</li>\n<li>默认值：column的name集合<br /></li>\n</ul>\n</li>\n<li><p><strong>fullColumnType</strong></p>\n<ul>\n<li>描述：写入的字段类型</li>\n<li>必须：否</li>\n<li>字段类型：list</li>\n<li>默认值：column的type集合<br /></li>\n</ul>\n</li>\n<li><p><strong>compress</strong></p>\n<ul>\n<li>描述：hdfs文件压缩类型<ul>\n<li>text：支持<code>GZIP</code>、<code>BZIP2</code>格式</li>\n<li>orc：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>BZIP</code>、<code>LZ4</code>格式</li>\n<li>parquet：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>LZO</code>格式</li>\n</ul>\n</li>\n<li>注意：<code>SNAPPY</code>格式需要用户安装<strong>SnappyCodec</strong></li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<ul>\n<li>text 默认不进行压缩</li>\n<li>orc 默认为ZLIB格式</li>\n<li>parquet 默认为SNAPPY格式<br /></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>maxFileSize</strong></p>\n<ul>\n<li>描述：写入hdfs单个文件最大大小，单位字节</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>1073741824</code>（1G）<br /></li>\n</ul>\n</li>\n<li><p><strong>nextCheckRows</strong></p>\n<ul>\n<li>描述：下一次检查文件大小的间隔条数，每达到该条数时会查询当前写入文件的文件大小</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>5000</code><br /></li>\n</ul>\n</li>\n<li><p><strong>rowGroupSIze</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时定row group的大小，单位字节</li>\n<li>必须：否</li>\n<li>字段类型：int</li>\n<li>默认值：<code>134217728</code>（128M）<br /></li>\n</ul>\n</li>\n<li><p><strong>enableDictionary</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时，是否启动字典编码</li>\n<li>必须：否</li>\n<li>字段类型：boolean</li>\n<li>默认值：<code>true</code><br /></li>\n</ul>\n</li>\n<li><p><strong>encoding</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的字符编码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>UTF-8</code></li>\n</ul>\n</li>\n</ul>\n\n              <a id=\"2、SQL\" style='display: block; height: 35px;'></a>\n              <h3>\n                2、SQL\n              </h3>\n            <ul>\n<li><p><strong>path</strong></p>\n<ul>\n<li>描述：写入的数据文件的路径</li>\n<li>注意：真正写入的文件路径是 path/fileName</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>file-name</strong></p>\n<ul>\n<li>描述：数据文件目录名称</li>\n<li>注意：真正写入的文件路径是 path/fileName</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>write-mode</strong></p>\n<ul>\n<li>描述：HDFS Sink写入前数据清理处理模式：<ul>\n<li>append：追加</li>\n<li>overwrite：覆盖</li>\n</ul>\n</li>\n<li>注意：overwrite模式时会删除hdfs当前目录下的所有文件</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：append<br /></li>\n</ul>\n</li>\n<li><p><strong>file-type</strong></p>\n<ul>\n<li>描述：文件的类型，目前只支持用户配置为<code>text</code>、<code>orc</code>、<code>parquet</code><ul>\n<li>text：textfile文件格式</li>\n<li>orc：orcfile文件格式</li>\n<li>parquet：parquet文件格式</li>\n</ul>\n</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>default-fs</strong></p>\n<ul>\n<li>描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>column</strong></p>\n<ul>\n<li>描述：需要读取的字段</li>\n<li>注意：不支持*格式</li>\n<li>格式：</li>\n</ul>\n</li>\n</ul>\n<pre><code class=\"language-text\"><span class=\"hljs-attr\">&quot;column&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-punctuation\">{</span>\n    <span class=\"hljs-attr\">&quot;name&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;col&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;type&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;string&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;index&quot;</span><span class=\"hljs-punctuation\">:</span><span class=\"hljs-number\">1</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;isPart&quot;</span><span class=\"hljs-punctuation\">:</span><span class=\"hljs-literal\"><span class=\"hljs-keyword\">false</span></span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;format&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;yyyy-MM-dd hh:mm:ss&quot;</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">&quot;value&quot;</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">&quot;value&quot;</span>\n<span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">]</span>\n</code></pre>\n<ul>\n<li><p>属性说明:</p>\n<ul>\n<li>name：必选，字段名称</li>\n<li>type：必选，字段类型，需要和数据文件中实际的字段类型匹配</li>\n<li>index：非必选，字段在所有字段中的位置索引，从0开始计算，默认为-1，按照数组顺序依次读取，配置后读取指定字段列</li>\n<li>isPart：非必选，是否是分区字段，如果是分区字段，会自动从path上截取分区赋值，默认为fale</li>\n<li>format：非必选，按照指定格式，格式化日期</li>\n<li>value：非必选，常量字段，将value的值作为常量列返回</li>\n</ul>\n</li>\n<li><p>必选：是</p>\n</li>\n<li><p>参数类型：数组</p>\n</li>\n<li><p>默认值：无</p>\n<br />\n</li>\n<li><p><strong>hadoopConfig</strong></p>\n<ul>\n<li>描述：集群HA模式时需要填写的core-site.xml及hdfs-site.xml中的配置，开启kerberos时包含kerberos相关配置</li>\n<li>必选：否</li>\n<li>配置方式：&#39;properties.key&#39; = &#39;value&#39;，key为hadoopConfig中的key，value为hadoopConfig中的value，如下所示：</li>\n</ul>\n</li>\n</ul>\n<pre><code><span class=\"hljs-attr\">&#x27;properties.hadoop.user.name&#x27;</span> = <span class=\"hljs-string\">&#x27;root&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.ha.namenodes.ns&#x27;</span> = <span class=\"hljs-string\">&#x27;nn1,nn2&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.defaultFS&#x27;</span> = <span class=\"hljs-string\">&#x27;hdfs://ns&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.namenode.rpc-address.ns.nn2&#x27;</span> = <span class=\"hljs-string\">&#x27;ip:9000&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.client.failover.proxy.provider.ns&#x27;</span> = <span class=\"hljs-string\">&#x27;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.namenode.rpc-address.ns.nn1&#x27;</span> = <span class=\"hljs-string\">&#x27;ip:9000&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.nameservices&#x27;</span> = <span class=\"hljs-string\">&#x27;ns&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.hdfs.impl.disable.cache&#x27;</span> = <span class=\"hljs-string\">&#x27;true&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.hdfs.impl&#x27;</span> = <span class=\"hljs-string\">&#x27;org.apache.hadoop.hdfs.DistributedFileSystem&#x27;</span>\n</code></pre>\n<ul>\n<li><p><strong>field-delimiter</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的分隔符</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：<code>\\001</code><br /></li>\n</ul>\n</li>\n<li><p><strong>compress</strong></p>\n<ul>\n<li>描述：hdfs文件压缩类型<ul>\n<li>text：支持<code>GZIP</code>、<code>BZIP2</code>格式</li>\n<li>orc：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>BZIP</code>、<code>LZ4</code>格式</li>\n<li>parquet：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>LZO</code>格式</li>\n</ul>\n</li>\n<li>注意：<code>SNAPPY</code>格式需要用户安装<strong>SnappyCodec</strong></li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<ul>\n<li>text 默认不进行压缩</li>\n<li>orc 默认为ZLIB格式</li>\n<li>parquet 默认为SNAPPY格式<br /></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>max-file-size</strong></p>\n<ul>\n<li>描述：写入hdfs单个文件最大大小，单位字节</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>1073741824</code>（1G）<br /></li>\n</ul>\n</li>\n<li><p><strong>next-check-rows</strong></p>\n<ul>\n<li>描述：下一次检查文件大小的间隔条数，每达到该条数时会查询当前写入文件的文件大小</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>5000</code><br /></li>\n</ul>\n</li>\n<li><p><strong>enable-dictionary</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时，是否启动字典编码</li>\n<li>必须：否</li>\n<li>字段类型：boolean</li>\n<li>默认值：<code>true</code><br /></li>\n</ul>\n</li>\n<li><p><strong>encoding</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的字符编码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>UTF-8</code></li>\n</ul>\n</li>\n<li><p><strong>sink.parallelism</strong></p>\n<ul>\n<li>描述：sink的并行度</li>\n<li>必选：否</li>\n<li>参数类型：String</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n</ul>\n\n              <a id=\"五、数据类型\" style='display: block; height: 35px;'></a>\n              <h2>\n                五、数据类型\n              </h2>\n            <table>\n<thead>\n<tr>\n<th>支持</th>\n<th>BOOLEAN、TINYINT、SMALLINT、INT、BIGINT、FLOAT、DOUBLE、DECIMAL、STRING、VARCHAR、CHAR、TIMESTAMP、DATE、BINARY</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>暂不支持</td>\n<td>ARRAY、MAP、STRUCT、UNION</td>\n</tr>\n</tbody></table>\n\n              <a id=\"六、脚本示例\" style='display: block; height: 35px;'></a>\n              <h2>\n                六、脚本示例\n              </h2>\n            <p>见项目内<code>chunjun-examples</code>文件夹。</p>\n","tree":[{"label":"ChunJun通用配置详解","category":"file"},{"label":"快速开始","category":"file"},{"label":"ChunJun拓展数据格式","children":[{"label":"protobuf-x","path":"protobuf-x","category":"file"}],"category":"dir"},{"label":"ChunJun连接器","children":[{"label":"binlog","category":"dir","children":[{"label":"binlog-source","path":"binlog-source","category":"file"}]},{"label":"clickhouse","category":"dir","children":[{"label":"clickhouse-lookup","path":"clickhouse-lookup","category":"file"},{"label":"clickhouse-sink","path":"clickhouse-sink","category":"file"},{"label":"clickhouse-source","path":"clickhouse-source","category":"file"}]},{"label":"db2","category":"dir","children":[{"label":"db2-lookup","path":"db2-lookup","category":"file"},{"label":"db2-sink","path":"db2-sink","category":"file"},{"label":"db2-source","path":"db2-source","category":"file"}]},{"label":"dm","category":"dir","children":[{"label":"dm-sink","path":"dm-sink","category":"file"},{"label":"dm-source","path":"dm-source","category":"file"}]},{"label":"doris","category":"dir","children":[{"label":"dorisbatch-sink","path":"dorisbatch-sink","category":"file"}]},{"label":"elasticsearch","category":"dir","children":[{"label":"es7-lookup","path":"es7-lookup","category":"file"},{"label":"es7-sink","path":"es7-sink","category":"file"},{"label":"es7-source","path":"es7-source","category":"file"}]},{"label":"gbase","category":"dir","children":[{"label":"gbase-lookup","path":"gbase-lookup","category":"file"},{"label":"gbase-sink","path":"gbase-sink","category":"file"},{"label":"gbase-source","path":"gbase-source","category":"file"}]},{"label":"greenplum","category":"dir","children":[{"label":"greenplum-sink","path":"greenplum-sink","category":"file"},{"label":"greenplum-source","path":"greenplum-source","category":"file"}]},{"label":"hbase","category":"dir","children":[{"label":"hbase-lookup","path":"hbase-lookup","category":"file"},{"label":"hbase-sink","path":"hbase-sink","category":"file"},{"label":"hbase-source","path":"hbase-source","category":"file"}]},{"label":"hdfs","category":"dir","children":[{"label":"hdfs-sink","path":"hdfs-sink","category":"file"},{"label":"hdfs-source","path":"hdfs-source","category":"file"}]},{"label":"hive","category":"dir","children":[{"label":"hive-lookup","path":"hive-lookup","category":"file"},{"label":"hive-sink","path":"hive-sink","category":"file"}]},{"label":"influxdb","category":"dir","children":[{"label":"influxdb-sink","path":"influxdb-sink","category":"file"},{"label":"influxdb-source","path":"influxdb-source","category":"file"}]},{"label":"kafka","category":"dir","children":[{"label":"kafka-sink","path":"kafka-sink","category":"file"},{"label":"kafka-source","path":"kafka-source","category":"file"}]},{"label":"kingbase","category":"dir","children":[{"label":"kingbase-sink","path":"kingbase-sink","category":"file"},{"label":"kingbase-source","path":"kingbase-source","category":"file"}]},{"label":"kudu","category":"dir","children":[{"label":"kudu-lookup","path":"kudu-lookup","category":"file"},{"label":"kudu-sink","path":"kudu-sink","category":"file"},{"label":"kudu-source","path":"kudu-source","category":"file"}]},{"label":"logminer","category":"dir","children":[{"label":"LogMiner-source","path":"LogMiner-source","category":"file"},{"label":"LogMiner原理","path":"LogMiner原理","category":"file"},{"label":"LogMiner配置","path":"LogMiner配置","category":"file"}]},{"label":"mongodb","category":"dir","children":[{"label":"mongodb-lookup","path":"mongodb-lookup","category":"file"},{"label":"mongodb-sink","path":"mongodb-sink","category":"file"},{"label":"mongodb-source","path":"mongodb-source","category":"file"}]},{"label":"mysql","category":"dir","children":[{"label":"mysql-lookup","path":"mysql-lookup","category":"file"},{"label":"mysql-sink","path":"mysql-sink","category":"file"},{"label":"mysql-source","path":"mysql-source","category":"file"}]},{"label":"oracle","category":"dir","children":[{"label":"oracle-lookup","path":"oracle-lookup","category":"file"},{"label":"oracle-sink","path":"oracle-sink","category":"file"},{"label":"oracle-source","path":"oracle-source","category":"file"}]},{"label":"pgwal","category":"dir","children":[{"label":"Postgres-CDC","path":"Postgres-CDC","category":"file"}]},{"label":"postgresql","category":"dir","children":[{"label":"postgres-lookup","path":"postgres-lookup","category":"file"},{"label":"postgres-sink","path":"postgres-sink","category":"file"},{"label":"postgres-source","path":"postgres-source","category":"file"}]},{"label":"rocketmq","category":"dir","children":[{"label":"rocketmq-source","path":"rocketmq-source","category":"file"}]},{"label":"saphana","category":"dir","children":[{"label":"saphana-sink","path":"saphana-sink","category":"file"},{"label":"saphana-source","path":"saphana-source","category":"file"}]},{"label":"sqlserver","category":"dir","children":[{"label":"sqlserver-lookup","path":"sqlserver-lookup","category":"file"},{"label":"sqlserver-sink","path":"sqlserver-sink","category":"file"},{"label":"sqlserver-source","path":"sqlserver-source","category":"file"}]},{"label":"sqlservercdc","category":"dir","children":[{"label":"SqlServer CDC实时采集原理","path":"SqlServer CDC实时采集原理","category":"file"},{"label":"SqlServer配置CDC","path":"SqlServer配置CDC","category":"file"},{"label":"SqlserverCDC-source","path":"SqlserverCDC-source","category":"file"}]}],"category":"dir"},{"label":"开发者指南","children":[{"label":"如何提交一个优秀的PR","path":"如何提交一个优秀的PR","category":"file"},{"label":"如何自定义插件","path":"如何自定义插件","category":"file"}],"category":"dir"},{"label":"拓展功能","children":[{"label":"增量同步介绍","path":"增量同步介绍","category":"file"},{"label":"断点续传介绍","path":"断点续传介绍","category":"file"},{"label":"脏数据插件设计","path":"脏数据插件设计","category":"file"}],"category":"dir"}],"toc":[{"text":"HDFS Sink","level":1,"id":"24c2229b-5b50-4d89-8744-66f0233bc4dd"},{"text":"一、介绍","level":2,"id":"a1df4e5c-adae-481d-b27f-7759e84998cb"},{"text":"二、支持版本","level":2,"id":"83151188-dbcf-44cd-99dc-d1a0d8ebb5c6"},{"text":"三、插件名称","level":2,"id":"96ae78b2-6941-41d4-bd0d-46a09a22af3e"},{"text":"四、参数说明","level":2,"id":"f2ab66d4-98e4-4bbd-bd55-b6f65caa73f4"},{"text":"1、Sync","level":3,"id":"dfaf5c0f-70f7-40f9-a634-a313127648db"},{"text":"2、SQL","level":3,"id":"ecfc6138-dce0-445f-ba3a-c9f12bfe7aac"},{"text":"五、数据类型","level":2,"id":"f4084793-6746-436c-864b-b64e464e1813"},{"text":"六、脚本示例","level":2,"id":"6114b043-f437-47ba-abdb-f78a2c026b8f"}]},"__N_SSG":true}