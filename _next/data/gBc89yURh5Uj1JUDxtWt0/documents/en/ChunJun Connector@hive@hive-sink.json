{"pageProps":{"content":"\n              <a id=\"Hive Sink\" style='display: block; height: 35px;'></a>\n              <h1>\n                Hive Sink\n              </h1>\n            \n              <a id=\"一、介绍\" style='display: block; height: 35px;'></a>\n              <h2>\n                一、介绍\n              </h2>\n            <p>ChunJun只有Hive sink插件，没有Hive source插件，如需要读取Hive表中的数据，请使用HDFS source插件。</p>\n<p>Hive sink插件支持实时地往Hive表中写数据，支持自动建表并根据当前系统时间自动创建分区，支持动态解析表名及分组映射，根据映射规则将不同的数据写入不同的Hive表。</p>\n<p>Hive sink插件一般配合实时采集(CDC)插件、kafka source插件等实时类的插件一起使用。</p>\n<p>Hive sink插件底层依赖HDFS sink，其基本原理也是向指定的HDFS路径中写入数据文件，可以看做是在HDFS sink上做了一些自动建表建分区及分组映射等拓展功能。</p>\n<p>Hive sink插件使用时需要开启checkpoint，在checkpoint后数据才能在Hive表中被查出。在开启checkpoint时会使用二阶段提交，预提交时将.data目录中生成的数据文件复制到正式目录中并标记复制的数据文件，提交阶段删除.data目录中标记的数据文件，回滚时删除正式目录中标记的数据文件。</p>\n\n              <a id=\"二、支持版本\" style='display: block; height: 35px;'></a>\n              <h2>\n                二、支持版本\n              </h2>\n            <p>Hive 1.x、Hive 2.x</p>\n\n              <a id=\"四、参数说明\" style='display: block; height: 35px;'></a>\n              <h2>\n                四、参数说明\n              </h2>\n            \n              <a id=\"1、Sync\" style='display: block; height: 35px;'></a>\n              <h3>\n                1、Sync\n              </h3>\n            <ul>\n<li><p><strong>jdbcUrl</strong></p>\n<ul>\n<li>描述：连接Hive JDBC的字符串</li>\n<li>必选：是</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>username</strong></p>\n<ul>\n<li>描述：Hive认证用户名</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>password</strong></p>\n<ul>\n<li>描述：Hive认证密码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>partition</strong></p>\n<ul>\n<li>描述：分区字段名称</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>pt</code><br /></li>\n</ul>\n</li>\n<li><p><strong>partitionType</strong></p>\n<ul>\n<li>描述：分区类型，包括 DAY、HOUR、MINUTE三种。<strong>若分区不存在则会自动创建，自动创建的分区时间以当前任务运行的服务器时间为准</strong><ul>\n<li>DAY：天分区，分区示例：pt=20200101</li>\n<li>HOUR：小时分区，分区示例：pt=2020010110</li>\n<li>MINUTE：分钟分区，分区示例：pt=202001011027</li>\n</ul>\n</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>DAY</code><br /></li>\n</ul>\n</li>\n<li><p><strong>tablesColumn</strong></p>\n<ul>\n<li>描述：写入hive表的表结构信息，<strong>若表不存在则会自动建表</strong>。</li>\n<li>示例：</li>\n<li>必选：是</li>\n<li>字段类型：string</li>\n<li>默认值：无</li>\n</ul>\n</li>\n</ul>\n<pre><code class=\"language-json\">{\n    <span class=\"hljs-string\">&quot;kudu&quot;</span>:[\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;user_id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;name&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;string&quot;</span>\n        }\n    ]\n}\n</code></pre>\n<br />\n\n<ul>\n<li><p><strong>analyticalRules</strong></p>\n<ul>\n<li>描述： 建表的动态规则获取表名，按照${XXXX}的占位符，从待写入数据(map结构)里根据key XXX 获取值进行替换，创建对应的表，并将数据写入对应的表</li>\n<li>示例：stream_${schema}_${table}</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>schema</strong></p>\n<ul>\n<li>描述： 自动建表时，analyticalRules里如果指定schema占位符，schema将此schema参数值进行替换</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>distributeTable</strong></p>\n<ul>\n<li>描述：如果数据来源于各个CDC数据，则将不同的表进行聚合，多张表的数据写入同一个hive表</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无</li>\n<li>示例：</li>\n</ul>\n</li>\n</ul>\n<pre><code class=\"language-json\"> <span class=\"hljs-string\">&quot;distributeTable&quot;</span> : <span class=\"hljs-string\">&quot;{<span class=\"hljs-subst\">\\&quot;</span>fenzu1<span class=\"hljs-subst\">\\&quot;</span>:[<span class=\"hljs-subst\">\\&quot;</span>table1<span class=\"hljs-subst\">\\&quot;</span>],<span class=\"hljs-subst\">\\&quot;</span>fenzu2<span class=\"hljs-subst\">\\&quot;</span>:[<span class=\"hljs-subst\">\\&quot;</span>table2<span class=\"hljs-subst\">\\&quot;</span>,<span class=\"hljs-subst\">\\&quot;</span>table3<span class=\"hljs-subst\">\\&quot;</span>]}&quot;</span>,\n</code></pre>\n<p>table1的数据将写入hive表fenzu1里，table2和table3的数据将写入fenzu2里,如果配置distributeTable，则tablesColumn需要配置为如下格式：</p>\n<pre><code class=\"language-json\">{\n    <span class=\"hljs-string\">&quot;fenzu1&quot;</span>:[\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;user_id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;name&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;string&quot;</span>\n        }\n    ],\n   <span class=\"hljs-string\">&quot;fenzu2&quot;</span>:[\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;user_id&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;int&quot;</span>\n        },\n        {\n            <span class=\"hljs-string\">&quot;key&quot;</span>:<span class=\"hljs-string\">&quot;name&quot;</span>,\n            <span class=\"hljs-string\">&quot;type&quot;</span>:<span class=\"hljs-string\">&quot;string&quot;</span>\n        }\n    ]\n}\n</code></pre>\n<br />\n\n<ul>\n<li><p><strong>writeMode</strong></p>\n<ul>\n<li>描述：HDFS Sink写入前数据清理处理模式：<ul>\n<li>append：追加</li>\n<li>overwrite：覆盖</li>\n</ul>\n</li>\n<li>注意：overwrite模式时会删除hdfs当前目录下的所有文件</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：append<br /></li>\n</ul>\n</li>\n<li><p><strong>fileType</strong></p>\n<ul>\n<li>描述：文件的类型，目前只支持用户配置为<code>text</code>、<code>orc</code>、<code>parquet</code><ul>\n<li>text：textfile文件格式</li>\n<li>orc：orcfile文件格式</li>\n<li>parquet：parquet文件格式</li>\n</ul>\n</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>defaultFS</strong></p>\n<ul>\n<li>描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>hadoopConfig</strong></p>\n<ul>\n<li>描述：集群HA模式时需要填写的core-site.xml及hdfs-site.xml中的配置，开启kerberos时包含kerberos相关配置</li>\n<li>必选：否</li>\n<li>参数类型：Map&lt;String, Object&gt;</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>fieldDelimiter</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的分隔符</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：<code>\\001</code><br /></li>\n</ul>\n</li>\n<li><p><strong>compress</strong></p>\n<ul>\n<li>描述：hdfs文件压缩类型<ul>\n<li>text：支持<code>GZIP</code>、<code>BZIP2</code>格式</li>\n<li>orc：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>BZIP</code>、<code>LZ4</code>格式</li>\n<li>parquet：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>LZO</code>格式</li>\n</ul>\n</li>\n<li>注意：<code>SNAPPY</code>格式需要用户安装<strong>SnappyCodec</strong></li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<ul>\n<li>text 默认不进行压缩</li>\n<li>orc 默认为ZLIB格式</li>\n<li>parquet 默认为SNAPPY格式<br /></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>maxFileSize</strong></p>\n<ul>\n<li>描述：写入hdfs单个文件最大大小，单位字节</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>1073741824‬</code>（1G）<br /></li>\n</ul>\n</li>\n<li><p><strong>nextCheckRows</strong></p>\n<ul>\n<li>描述：下一次检查文件大小的间隔条数，每达到该条数时会查询当前写入文件的文件大小</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>5000</code><br /></li>\n</ul>\n</li>\n<li><p><strong>rowGroupSize</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时定row group的大小，单位字节</li>\n<li>必须：否</li>\n<li>字段类型：int</li>\n<li>默认值：<code>134217728</code>（128M）<br /></li>\n</ul>\n</li>\n<li><p><strong>enableDictionary</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时，是否启动字典编码</li>\n<li>必须：否</li>\n<li>字段类型：boolean</li>\n<li>默认值：<code>true</code><br /></li>\n</ul>\n</li>\n<li><p><strong>encoding</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的字符编码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>UTF-8</code><br /></li>\n</ul>\n</li>\n</ul>\n\n              <a id=\"2、SQL\" style='display: block; height: 35px;'></a>\n              <h3>\n                2、SQL\n              </h3>\n            <ul>\n<li><p><strong>url</strong></p>\n<ul>\n<li>描述：连接Hive JDBC的字符串</li>\n<li>必选：是</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>username</strong></p>\n<ul>\n<li>描述：Hive认证用户名</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>password</strong></p>\n<ul>\n<li>描述：Hive认证密码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>partition</strong></p>\n<ul>\n<li>描述：分区字段名称</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>pt</code><br /></li>\n</ul>\n</li>\n<li><p><strong>partition-type</strong></p>\n<ul>\n<li>描述：分区类型，包括 DAY、HOUR、MINUTE三种。<strong>若分区不存在则会自动创建，自动创建的分区时间以当前任务运行的服务器时间为准</strong><ul>\n<li>DAY：天分区，分区示例：pt=20200101</li>\n<li>HOUR：小时分区，分区示例：pt=2020010110</li>\n<li>MINUTE：分钟分区，分区示例：pt=202001011027</li>\n</ul>\n</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>DAY</code><br /></li>\n</ul>\n</li>\n<li><p><strong>write-mode</strong></p>\n<ul>\n<li>描述：HDFS Sink写入前数据清理处理模式：<ul>\n<li>append：追加</li>\n<li>overwrite：覆盖</li>\n</ul>\n</li>\n<li>注意：overwrite模式时会删除hdfs当前目录下的所有文件</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：append<br /></li>\n</ul>\n</li>\n<li><p><strong>file-type</strong></p>\n<ul>\n<li>描述：文件的类型，目前只支持用户配置为<code>text</code>、<code>orc</code>、<code>parquet</code><ul>\n<li>text：textfile文件格式</li>\n<li>orc：orcfile文件格式</li>\n<li>parquet：parquet文件格式</li>\n</ul>\n</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>default-fs</strong></p>\n<ul>\n<li>描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000</li>\n<li>必选：是</li>\n<li>参数类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n<li><p><strong>hadoopConfig</strong></p>\n<ul>\n<li>描述：集群HA模式时需要填写的core-site.xml及hdfs-site.xml中的配置，开启kerberos时包含kerberos相关配置</li>\n<li>必选：否</li>\n<li>配置方式：&#39;properties.key&#39; = &#39;value&#39;，key为hadoopConfig中的key，value为hadoopConfig中的value，如下所示：</li>\n</ul>\n</li>\n</ul>\n<pre><code><span class=\"hljs-attr\">&#x27;properties.hadoop.user.name&#x27;</span> = <span class=\"hljs-string\">&#x27;root&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.ha.namenodes.ns&#x27;</span> = <span class=\"hljs-string\">&#x27;nn1,nn2&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.defaultFS&#x27;</span> = <span class=\"hljs-string\">&#x27;hdfs://ns&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.namenode.rpc-address.ns.nn2&#x27;</span> = <span class=\"hljs-string\">&#x27;ip:9000&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.client.failover.proxy.provider.ns&#x27;</span> = <span class=\"hljs-string\">&#x27;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.namenode.rpc-address.ns.nn1&#x27;</span> = <span class=\"hljs-string\">&#x27;ip:9000&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.dfs.nameservices&#x27;</span> = <span class=\"hljs-string\">&#x27;ns&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.hdfs.impl.disable.cache&#x27;</span> = <span class=\"hljs-string\">&#x27;true&#x27;</span>,\n<span class=\"hljs-attr\">&#x27;properties.fs.hdfs.impl&#x27;</span> = <span class=\"hljs-string\">&#x27;org.apache.hadoop.hdfs.DistributedFileSystem&#x27;</span>\n</code></pre>\n<ul>\n<li><p><strong>field-delimiter</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的分隔符</li>\n<li>必选：否</li>\n<li>参数类型：string</li>\n<li>默认值：<code>\\001</code><br /></li>\n</ul>\n</li>\n<li><p><strong>compress</strong></p>\n<ul>\n<li>描述：hdfs文件压缩类型<ul>\n<li>text：支持<code>GZIP</code>、<code>BZIP2</code>格式</li>\n<li>orc：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>BZIP</code>、<code>LZ4</code>格式</li>\n<li>parquet：支持<code>SNAPPY</code>、<code>GZIP</code>、<code>LZO</code>格式</li>\n</ul>\n</li>\n<li>注意：<code>SNAPPY</code>格式需要用户安装<strong>SnappyCodec</strong></li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<ul>\n<li>text 默认不进行压缩</li>\n<li>orc 默认为ZLIB格式</li>\n<li>parquet 默认为SNAPPY格式<br /></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>max-file-size</strong></p>\n<ul>\n<li>描述：写入hdfs单个文件最大大小，单位字节</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>1073741824</code>（1G）<br /></li>\n</ul>\n</li>\n<li><p><strong>next-check-rows</strong></p>\n<ul>\n<li>描述：下一次检查文件大小的间隔条数，每达到该条数时会查询当前写入文件的文件大小</li>\n<li>必选：否</li>\n<li>字段类型：long</li>\n<li>默认值：<code>5000</code><br /></li>\n</ul>\n</li>\n<li><p><strong>enable-dictionary</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>parquet</code>时，是否启动字典编码</li>\n<li>必须：否</li>\n<li>字段类型：boolean</li>\n<li>默认值：<code>true</code><br /></li>\n</ul>\n</li>\n<li><p><strong>encoding</strong></p>\n<ul>\n<li>描述：<code>fileType</code>为<code>text</code>时字段的字符编码</li>\n<li>必选：否</li>\n<li>字段类型：string</li>\n<li>默认值：<code>UTF-8</code><br /></li>\n</ul>\n</li>\n<li><p><strong>table-name</strong></p>\n<ul>\n<li>描述：Hive表名</li>\n<li>必选：是</li>\n<li>字段类型：string</li>\n<li>默认值：无<br /></li>\n</ul>\n</li>\n</ul>\n\n              <a id=\"五、数据类型\" style='display: block; height: 35px;'></a>\n              <h2>\n                五、数据类型\n              </h2>\n            <table>\n<thead>\n<tr>\n<th>支持</th>\n<th>BOOLEAN、TINYINT、SMALLINT、INT、BIGINT、FLOAT、DOUBLE、DECIMAL、STRING、VARCHAR、CHAR、TIMESTAMP、DATE、BINARY</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>暂不支持</td>\n<td>ARRAY、MAP、STRUCT、UNION</td>\n</tr>\n</tbody></table>\n\n              <a id=\"六、脚本示例\" style='display: block; height: 35px;'></a>\n              <h2>\n                六、脚本示例\n              </h2>\n            <p>见项目内<code>chunjun-examples</code>文件夹。</p>\n","tree":[{"label":"General Configuration","category":"file"},{"label":"Quick Start","category":"file"},{"label":"Advanced Features","children":[{"label":"增量同步介绍","path":"增量同步介绍","category":"file"},{"label":"断点续传介绍","path":"断点续传介绍","category":"file"},{"label":"脏数据插件设计","path":"脏数据插件设计","category":"file"}],"category":"dir"},{"label":"ChunJun Connector","children":[{"label":"binlog","category":"dir","children":[{"label":"binlog-source","path":"binlog-source","category":"file"}]},{"label":"clickhouse","category":"dir","children":[{"label":"clickhouse-lookup","path":"clickhouse-lookup","category":"file"},{"label":"clickhouse-sink","path":"clickhouse-sink","category":"file"},{"label":"clickhouse-source","path":"clickhouse-source","category":"file"}]},{"label":"db2","category":"dir","children":[{"label":"db2-lookup","path":"db2-lookup","category":"file"},{"label":"db2-sink","path":"db2-sink","category":"file"},{"label":"db2-source","path":"db2-source","category":"file"}]},{"label":"dm","category":"dir","children":[{"label":"dm-sink","path":"dm-sink","category":"file"},{"label":"dm-source","path":"dm-source","category":"file"}]},{"label":"doris","category":"dir","children":[{"label":"dorisbatch-sink","path":"dorisbatch-sink","category":"file"}]},{"label":"elasticsearch","category":"dir","children":[{"label":"es7-lookup","path":"es7-lookup","category":"file"},{"label":"es7-sink","path":"es7-sink","category":"file"},{"label":"es7-source","path":"es7-source","category":"file"}]},{"label":"gbase","category":"dir","children":[{"label":"gbase-lookup","path":"gbase-lookup","category":"file"},{"label":"gbase-sink","path":"gbase-sink","category":"file"},{"label":"gbase-source","path":"gbase-source","category":"file"}]},{"label":"greenplum","category":"dir","children":[{"label":"greenplum-sink","path":"greenplum-sink","category":"file"},{"label":"greenplum-source","path":"greenplum-source","category":"file"}]},{"label":"hbase","category":"dir","children":[{"label":"hbase-lookup","path":"hbase-lookup","category":"file"},{"label":"hbase-sink","path":"hbase-sink","category":"file"},{"label":"hbase-source","path":"hbase-source","category":"file"}]},{"label":"hdfs","category":"dir","children":[{"label":"hdfs-sink","path":"hdfs-sink","category":"file"},{"label":"hdfs-source","path":"hdfs-source","category":"file"}]},{"label":"hive","category":"dir","children":[{"label":"hive-lookup","path":"hive-lookup","category":"file"},{"label":"hive-sink","path":"hive-sink","category":"file"}]},{"label":"influxdb","category":"dir","children":[{"label":"influxdb-sink","path":"influxdb-sink","category":"file"},{"label":"influxdb-source","path":"influxdb-source","category":"file"}]},{"label":"kafka","category":"dir","children":[{"label":"kafka-sink","path":"kafka-sink","category":"file"},{"label":"kafka-source","path":"kafka-source","category":"file"}]},{"label":"kingbase","category":"dir","children":[{"label":"kingbase-sink","path":"kingbase-sink","category":"file"},{"label":"kingbase-source","path":"kingbase-source","category":"file"}]},{"label":"kudu","category":"dir","children":[{"label":"kudu-lookup","path":"kudu-lookup","category":"file"},{"label":"kudu-sink","path":"kudu-sink","category":"file"},{"label":"kudu-source","path":"kudu-source","category":"file"}]},{"label":"logminer","category":"dir","children":[{"label":"LogMiner-source","path":"LogMiner-source","category":"file"},{"label":"LogMiner原理","path":"LogMiner原理","category":"file"},{"label":"LogMiner配置","path":"LogMiner配置","category":"file"}]},{"label":"mongodb","category":"dir","children":[{"label":"mongodb-lookup","path":"mongodb-lookup","category":"file"},{"label":"mongodb-sink","path":"mongodb-sink","category":"file"},{"label":"mongodb-source","path":"mongodb-source","category":"file"}]},{"label":"mysql","category":"dir","children":[{"label":"mysql-lookup","path":"mysql-lookup","category":"file"},{"label":"mysql-sink","path":"mysql-sink","category":"file"},{"label":"mysql-source","path":"mysql-source","category":"file"}]},{"label":"oracle","category":"dir","children":[{"label":"oracle-lookup","path":"oracle-lookup","category":"file"},{"label":"oracle-sink","path":"oracle-sink","category":"file"},{"label":"oracle-source","path":"oracle-source","category":"file"}]},{"label":"pgwal","category":"dir","children":[{"label":"Postgres-CDC","path":"Postgres-CDC","category":"file"}]},{"label":"postgresql","category":"dir","children":[{"label":"postgres-lookup","path":"postgres-lookup","category":"file"},{"label":"postgres-sink","path":"postgres-sink","category":"file"},{"label":"postgres-source","path":"postgres-source","category":"file"}]},{"label":"rocketmq","category":"dir","children":[{"label":"rocketmq-source","path":"rocketmq-source","category":"file"}]},{"label":"saphana","category":"dir","children":[{"label":"saphana-sink","path":"saphana-sink","category":"file"},{"label":"saphana-source","path":"saphana-source","category":"file"}]},{"label":"sqlserver","category":"dir","children":[{"label":"sqlserver-lookup","path":"sqlserver-lookup","category":"file"},{"label":"sqlserver-sink","path":"sqlserver-sink","category":"file"},{"label":"sqlserver-source","path":"sqlserver-source","category":"file"}]},{"label":"sqlservercdc","category":"dir","children":[{"label":"SqlServer CDC实时采集原理","path":"SqlServer CDC实时采集原理","category":"file"},{"label":"SqlServer配置CDC","path":"SqlServer配置CDC","category":"file"},{"label":"SqlserverCDC-source","path":"SqlserverCDC-source","category":"file"}]}],"category":"dir"},{"label":"ChunJun Extend Data Format","children":[{"label":"protobuf-x","path":"protobuf-x","category":"file"}],"category":"dir"},{"label":"Contribution","children":[{"label":"How to define a plugin","path":"How to define a plugin","category":"file"},{"label":"How to submit a great Pull Request","path":"How to submit a great Pull Request","category":"file"}],"category":"dir"}],"toc":[{"text":"Hive Sink","level":1,"id":"34b5fe57-8952-4177-8da6-d149b49dd575"},{"text":"一、介绍","level":2,"id":"4cd84242-5db4-4a0b-9376-5cfca3850922"},{"text":"二、支持版本","level":2,"id":"57c56ef9-5fba-498e-b5c1-f0a4d15112c6"},{"text":"四、参数说明","level":2,"id":"75d8c489-27cf-45cc-b38f-c7226070bfce"},{"text":"1、Sync","level":3,"id":"d23f3e1b-3da0-42d6-9a11-7eb73c71308d"},{"text":"2、SQL","level":3,"id":"9f4824df-8c97-4a82-8e9c-e820b8cf09fe"},{"text":"五、数据类型","level":2,"id":"f8cb5be3-8f47-4b83-9a82-9f07740706bd"},{"text":"六、脚本示例","level":2,"id":"3c9bb92c-0d30-4a99-86c5-3f2ff85a0f10"}]},"__N_SSG":true}